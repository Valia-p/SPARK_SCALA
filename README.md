# SPARK_SCALA

This repository contains two Apache Spark applications written in **Scala**:
- **Exercise 1** using the **RDD API**
- **Exercise 2** using the **DataFrame API**

The repository also includes the output files produced by each exercise.

---

## Contents

- `Exercise1.scala` — Spark RDD application
- `Exercise2.scala` — Spark DataFrame application
- Output files in `txt` / `csv` format (Spark output folders with `part-*` files)

---

## Requirements

- Java 11  
- Scala 2.13.x  
- Apache Spark 3.x (via project dependencies)

---

## Exercise 1 — RDD API (Sherlock Holmes Text Analysis)

### Description
The program reads the `SherlockHolmes.txt` file and computes, for each starting letter (`a–z`), the **average word length** of the words that begin with that letter.  
The results are sorted in **descending order** by average word length.

### Data Cleaning
- Removes punctuation characters
- Converts all text to lowercase
- Keeps only words that start with a letter (`[a-z].*`)
- Ignores words that start with numbers

### Output
Saved in:
- `outputs/output1/`

Each line has the form:
```text
letter,average_length
```
## Exercise 2 — DataFrame API (Twitter Data Analysis)

### Objective
The goal of this exercise is to analyze Twitter data related to airline services using **Apache Spark’s DataFrame API**.  
We act as data analysts and extract useful insights that could help airlines improve the quality of their services.

The input is a CSV file where each row represents a tweet and includes information about sentiment, airline, complaint reasons, and the tweet text.

---

### Input Data
The dataset (`tweets.csv`) contains the following columns:

- `tweet_id`
- `airline_sentiment` (positive, negative, neutral)
- `airline_sentiment_confidence`
- `negativereason`
- `negativereason_confidence`
- `airline`
- `name`
- `text`
- `tweet_created`
- `user_timezone`

The input file is read from the **local file system**, as required by the assignment.

---

### Data Preprocessing
Before performing the analysis, the following preprocessing steps are applied:

- Convert all text to **lowercase**
- Remove **punctuation characters**
- Split tweet text into individual words
- Ignore empty tokens
- Consider only valid sentiment values: `positive`, `negative`, `neutral`

These steps ensure consistency and accuracy in the word frequency analysis.

---

### Task 1: Top 5 Words per Sentiment
For each sentiment category (`positive`, `negative`, `neutral`):

1. The tweet text is cleaned and tokenized into words.
2. Words are counted per sentiment.
3. A window function is used to rank words by frequency.
4. The **top 5 most frequent words** are selected for each sentiment.

The results are:
- Printed to the console
- Saved to disk in the `outputs/topwords` directory

Each output line contains: <airline_sentiment>, <word>, <count>


---

### Task 2: Main Complaint Reason per Airline
This task focuses on identifying the most common complaint reason for each airline.

Steps:
1. Filter tweets where:
   - `negativereason` is not null
   - `airline` is not null
   - `negativereason_confidence > 0.5`
2. Group data by `airline` and `negativereason`
3. Count the number of tweets per group
4. Use a window function to select the **most frequent complaint reason** per airline

The results are:
- Printed to the console
- Saved to disk in the `outputs/topreason` directory

Each output line contains: <airline>, <negativereason>, <count>

---

### Output
The output of Exercise 2 is stored in two directories:

- `outputs/topwords/` — top 5 words per sentiment
- `outputs/topreason/` — main complaint reason per airline

The results are stored as text/CSV-style files generated by Spark.

---

### Technologies Used
- Apache Spark 3.x
- Spark SQL / DataFrame API
- Scala 2.13
- Local file system execution (`local[*]`)

---
